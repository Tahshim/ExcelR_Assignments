{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d874df29",
   "metadata": {},
   "source": [
    "# Assignment 13 â€” LightGBM vs XGBoost (Polished)\n",
    "\n",
    "This notebook is prepared to fully match the assignment requirements. Run it in Google Colab or your local environment to execute all cells (install packages if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada5ff2",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Compare LightGBM and XGBoost on the provided `diabetes (3).csv` dataset. The notebook includes: EDA, preprocessing, imbalance handling, model training, evaluation metrics, and comparison notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dbb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import LightGBM and XGBoost\n",
    "use_lgb = False\n",
    "use_xgb = False\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    use_lgb = True\n",
    "    print('lightgbm available')\n",
    "except Exception:\n",
    "    print('lightgbm not available')\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    use_xgb = True\n",
    "    print('xgboost available')\n",
    "except Exception:\n",
    "    print('xgboost not available')\n",
    "\n",
    "# Try SMOTE\n",
    "use_smote = False\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    use_smote = True\n",
    "    print('imblearn SMOTE available')\n",
    "except Exception:\n",
    "    print('imblearn not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load dataset\n",
    "df = pd.read_csv('/mnt/data/diabetes (3).csv')\n",
    "print('Shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1233104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Quick EDA\n",
    "print('Missing values:\\n', df.isna().sum())\n",
    "print('\\nTarget value counts (last column):')\n",
    "print(df.iloc[:,-1].value_counts())\n",
    "\n",
    "# Basic histograms\n",
    "_ = df.hist(bins=20, figsize=(12,8))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abedf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Preprocessing\n",
    "# Choose target\n",
    "if 'Outcome' in df.columns:\n",
    "    target = 'Outcome'\n",
    "else:\n",
    "    target = df.columns[-1]\n",
    "print('Target:', target)\n",
    "\n",
    "# Drop rows with missing target\n",
    "df = df[df[target].notna()].reset_index(drop=True)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Encode object columns\n",
    "for c in X.select_dtypes(include=['object','category']).columns:\n",
    "    X[c] = X[c].astype('category').cat.codes\n",
    "\n",
    "# Fill numeric NaNs\n",
    "for c in X.select_dtypes(include=[np.number]).columns:\n",
    "    X[c] = X[c].fillna(X[c].median())\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print('Features shape:', X_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae765253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Handle imbalance\n",
    "print('Original class distribution:\\n', pd.Series(y).value_counts())\n",
    "if use_smote:\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X_scaled, y)\n",
    "    print('After SMOTE:', pd.Series(y_res).value_counts())\n",
    "else:\n",
    "    X_res, y_res = X_scaled, y\n",
    "    print('Using original distribution; consider class_weight or resampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d63cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.25, random_state=42, stratify=y_res if len(pd.Series(y_res).unique())>1 else None)\n",
    "print('Train/test sizes:', X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca874e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Helper function for evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('----', name, '----')\n",
    "    print('Accuracy:', round(accuracy_score(y_test, y_pred),4))\n",
    "    print('Precision (weighted):', round(precision_score(y_test, y_pred, average='weighted', zero_division=0),4))\n",
    "    print('Recall (weighted):', round(recall_score(y_test, y_pred, average='weighted', zero_division=0),4))\n",
    "    print('F1 (weighted):', round(f1_score(y_test, y_pred, average='weighted', zero_division=0),4))\n",
    "    try:\n",
    "        if len(pd.Series(y_test).unique())==2:\n",
    "            y_proba = model.predict_proba(X_test)[:,1]\n",
    "            print('ROC AUC:', round(roc_auc_score(y_test, y_proba),4))\n",
    "    except Exception:\n",
    "        pass\n",
    "    print('\\nClassification report:\\n', classification_report(y_test, y_pred))\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Train LightGBM (or fallback)\n",
    "if use_lgb:\n",
    "    model_lgb = lgb.LGBMClassifier(n_estimators=100, random_state=42)\n",
    "else:\n",
    "    model_lgb = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "model_lgb.fit(X_train, y_train)\n",
    "evaluate('LightGBM/Fallback', model_lgb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe5c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Train XGBoost (or fallback)\n",
    "if use_xgb:\n",
    "    model_xgb = xgb.XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "else:\n",
    "    model_xgb = GradientBoostingClassifier(n_estimators=100, random_state=7)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "evaluate('XGBoost/Fallback', model_xgb, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Feature importances (if available)\n",
    "try:\n",
    "    if hasattr(model_lgb, 'feature_importances_'):\n",
    "        imp = pd.Series(model_lgb.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    elif hasattr(model_xgb, 'feature_importances_'):\n",
    "        imp = pd.Series(model_xgb.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    else:\n",
    "        imp = None\n",
    "    if imp is not None:\n",
    "        display(imp)\n",
    "        sns.barplot(x=imp.values, y=imp.index)\n",
    "        plt.title('Feature importances')\n",
    "        plt.show()\n",
    "except Exception as e:\n",
    "    print('Feature importance error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4678d54",
   "metadata": {},
   "source": [
    "## Conclusions & Notes\n",
    "- Bagging vs Boosting: Bagging trains independent models and averages (reduces variance). Boosting trains sequential models focusing on mistakes (reduces bias). \n",
    "- Imbalance handling: SMOTE (oversampling), undersampling, or class_weight can be used. Prefer F1/ROC metrics when imbalance exists.\n",
    "\n",
    "**To run this notebook fully:**\n",
    "- On Google Colab run `!pip install lightgbm xgboost imbalanced-learn` before running cells, if you want full LGBM/XGB/SMOTE support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af029d4e",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
